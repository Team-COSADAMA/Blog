---

title: í•œêµ­ì–´ ì„ë² ë”© (ë¬¸ì¥ ìˆ˜ì¤€) 
description: ì‹¤ì „NLP_ë‹¤ì„¯ ë²ˆì§¸
slug: 2021-practiceNLP-05
category: Deep-Learning
author: ì´ì •ìœ¤

---

## ğŸŒŠ Introduction

ì´ì „ ìŠ¤í„°ë””ì—ì„œ ì§„í–‰í•œ ë‹¨ì–´ê¸°ë°˜ ì„ë² ë”© ê¸°ë²•ì„ í™œìš©í–ˆì„ ë•Œ ê½¤ ê·¸ëŸ´ë“¯í•œ ìœ ì‚¬ë„ ì¶”ì¶œì´ ì§„í–‰ë˜ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ë­‰ì¹˜ ì•ˆì— ì—†ëŠ” ë‹¨ì–´ì˜ ìœ ì‚¬ë„ëŠ” ì°¾ì§€ ëª»í•˜ë©° ë™ìŒì´ì˜ì–´ëŠ” êµ¬ë¶„í•˜ì§€ ëª»í•œë‹¤ëŠ” í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´ 'ì˜ì‚¬' (doctor)ì™€ 'ì˜ì‚¬'(patriot)ëŠ” êµ¬ë¶„í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. 

ì´ëŸ° í•œê³„ëŠ” <u>ë¬¸ì¥ê¸°ë°˜ ì„ë² ë”©</u>ë¥¼ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ë¬¸ì¥ê¸°ë°˜ ì„ë² ë”©ì€ ì•ì„œ ì„¤ëª…í•œ ë‹¨ì–´ê¸°ë°˜ ì„ë² ë”© ê¸°ë²•ì˜ í•œê³„ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œ ìµœê·¼ ë§ì´ ì—°êµ¬ë˜ê³  ìˆë‹¤. ê·¸ ì¤‘ì—ì„œë„ ì˜ˆì¸¡ ê¸°ë°˜ì¸ BERTì™€ ElMo, ê·¸ë¦¬ê³  í† í”½ ê¸°ë°˜ ì„ë² ë”© ê¸°ë²•ì¸ LDAë¥¼ ì‹¤ìŠµí•´ ë³¼ ì˜ˆì •ì´ë‹¤. ì„ë² ë”© ê¸°ë²• ì°¨ì´ ë° ë¶„ë¥˜ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ì´ì „ [3ì£¼ì°¨ í¬ìŠ¤íŒ…](https://www.blog.cosadama.com/2021-practiceNLP-03)ì„ ì°¸ê³ í•˜ê¸¸ ë°”ë€ë‹¤. 

í•œí¸, EMLoì™€ BERT ê°™ì€ ê²½ìš°ì—ëŠ” ì•„ì§ ë”°ëˆë”°ëˆí•œ ìƒˆë¡œìš´ ê¸°ë²•ì¸ ë§Œí¼ ì‹¤ìŠµ ì˜ˆì œê°€ ë§ì§€ ì•Šë‹¤. (ë˜í•œ pretrainí•  ë•Œ í•„ìš”í•œ ë°ì´í„°ì˜ í¬ê¸°ë„ ê°€ëŠ í•  ìˆ˜ ì—†ë‹¤)ë‘ ê¸°ë²•ì— ëŒ€í•œ ì´í•´ê°€ ë¶€ì¡±í•œ ìƒíƒœì—ì„œ ë‚´ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ì ìš©í•˜ê¸° ì‰½ì§€ ì•Šì•„ì„œ ì´ë²ˆ ì‹¤ìŠµë§Œí¼ì€ ê¸°ì¡´ì˜ ì˜ˆì‹œ ì½”ë“œë¥¼ í™œìš©í•˜ì—¬ ì§„í–‰í•˜ë„ë¡ í•œë‹¤. 

ì‹¤ìŠµì— ì‚¬ìš©í•œ ëª¨ë“  ì½”ë“œëŠ” [ì—¬ê¸°](https://colab.research.google.com/drive/1sb4DXAVQ7X5ICXec-RsGF6AePx4eaBgH)ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

ì°¸ê³ ë„ì„œ: í•œêµ­ì–´ì„ë² ë”©(ì´ê¸°ì°½, 2019)     
ì°¸ê³ ì‚¬ì´íŠ¸: [ìœ„í‚¤ë…ìŠ¤-ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/22644)

## ğŸŒŠ Preprocessing & Tokenization
LDAì—ì„œ ì‚¬ìš©í•œ ë§ë­‰ì¹˜ëŠ” 'ì²­ì™€ëŒ€ ì²­ì›' í¬ë¡¤ë§ ë‚´ìš©ì´ë‹¤. ì „ì²´ ì²­ì›ì„ í¬ë¡¤ë§í•˜ê¸°ì—ëŠ” ì‹œê°„ì , ê¸°ê³„ ì„±ëŠ¥ì ìœ¼ë¡œ í•œê³„ê°€ ìˆì–´ ìµœê·¼ 3ê°œì›” ë°ì´í„°ë§Œ ìˆ˜ì§‘í–ˆë‹¤. 

í† í°í™”ëŠ” [1ì£¼ì°¨ ìŠ¤í„°ë””](https://www.blog.cosadama.com/2021-practiceNLP-01)ì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë˜ __Mecab__ ì„ ì‚¬ìš©í•œë‹¤. (ë¶ˆìš©ì–´ë¥¼ ì¡°ê¸ˆ ë” ì •êµí•˜ê²Œ ì œê±°í•´ ì¤„ í•„ìš”ê°€ ìˆë‹¤.)     
í† í°í™” í›„  ì¼ë¶€ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” ë“±ì˜ ì „ì²˜ë¦¬ ë¶€ë¶„ì€ ì´ë²ˆ ìŠ¤í„°ë””ì˜ ì£¼ ë‚´ìš©ì´ ì•„ë‹ˆë¯€ë¡œ [ì½”ë©íŒŒì¼](https://colab.research.google.com/drive/1sb4DXAVQ7X5ICXec-RsGF6AePx4eaBgH#scrollTo=7HZIHs1A--10&line=1&uniqifier=1)ì—ì„œ ì§„í–‰í•œ ê²ƒìœ¼ë¡œ ëŒ€ì²´í•˜ë„ë¡ í•˜ê² ë‹¤. 

##  ğŸŒŠ Embedding

### 1. LDA
LDA(Latent Dirichlet Allocation)ì€ ì£¼ì–´ì§„ ë¬¸ì„œì— ëŒ€í•˜ì—¬ ê° ë¬¸ì„œì— ì–´ë–¤ í† í”½(ì£¼ì œ)ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ì— ëŒ€í•œ í™•ë¥  ëª¨í˜•ì´ë©° ì£¼ì œë¥¼ ì¶”ì¶œí•œë‹¤ëŠ” ì ì—ì„œ 'í† í”½ ëª¨ë¸ë§'ì´ë¼ê³ ë„ í•œë‹¤. (í•œêµ­ì–´ ì„ë² ë”©, 2019, p190) ì´ë•Œ íŠ¹ì • í† í”½ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ëª¨ì„ ìˆ˜ëŠ” ìˆì§€ë§Œ, í•´ë‹¹ ë‚´ìš©ì´ ì–´ë–¤ í† í”½, ì¦‰ ì¤‘ì‹¬ ì£¼ì œ ë‚´ìš©ì„ ë½‘ì•„ë‚´ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. 

1. ëª¨ë“ˆì„¤ì¹˜
```
!pip install pyLDAvis
!pip install --upgrade gensim
## ì‹œê°í™” íˆ´
!pip install pyLDAvis
```
```
from gensim import corpora, models
import gensim
import pandas as pd
```
```
#ì‹œê°í™” íˆ´
import pyLDAvis
import pyLDAvis.gensim_models
```
2. í•™ìŠµí•˜ê¸°
```
# í† í°í™”ëœ ë§ë­‰ì¹˜ëŠ” tokenized_data ë³€ìˆ˜ ì‚¬ìš©

## ë‹¨ì–´í•™ìŠµí•˜ê¸°
dictionary = corpora.Dictionary(tokenized_data)
## ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬(document-term matrix) ìƒì„±
corpus = [dictionary.doc2bow(term) for term in tokenized_data]
model = models.ldamodel.LdaModel(corpus, num_topics=9, id2word = dictionary)
model.show_topics(4, 10)
```
```
NUM_TOPICS = 9 #í† í”½ê°œìˆ˜

word_dict = {}
for i in  range(NUM_TOPICS):
	words = model.show_topic(i, topn=20)
	word_dict['Topic #' + '{:02d}'.format(i+1)] = [i[0] for i in words]
	word_df = pd.DataFrame(word_dict)
```
ì „ì²´ ë°ì´í„°ë¥¼ ì´ 9ê°œì˜ ì£¼ì œë¡œ ë¶„ë¥˜í•œë‹¤. (NUM_TOPICS=9)   
ë¶„ë¥˜ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. 
![LDA_1](/practiceNLP/LDA_1.png)
í† í”½1~9ê¹Œì§€ ì‚´í´ë³´ë©´ ë¹„ìŠ·í•œ ì¹´í…Œê³ ë¦¬ì˜ ë‹¨ì–´ë¼ë¦¬ ë­‰ì³ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ í† í”½ë¼ë¦¬ ê²¹ì¹˜ëŠ” ë‹¨ì–´ë„ ë§ê³ , ì •í™•íˆ ì–´ë–¤ ì£¼ì œì˜ í† í”½ìœ¼ë¡œ ë¶„ë¥˜ëœ ê²ƒì¸ì§€ í™•ì¸í•˜ê¸° ì• ë§¤í•œ ê²½ìš°ê°€ ìˆë‹¤. ì´ëŠ” LDAì˜ í•œê³„ì´ê¸°ë„ í•˜ê³ , ì‚¬ìš©í•œ ë§ë­‰ì¹˜ì˜ ì£¼ì œìˆ˜ê°€ ê³ ë¥´ì§€ ì•Šì•„(ì´ë•ŒëŠ” 'ë³´ê±´ë³µì§€' ì¹´í…Œê³ ë¦¬ì˜ ì²­ì›ìˆ˜ê°€ ì›”ë“±íˆ ë§ì•˜ë‹¤.) ë¶„ë¥˜ê²°ê³¼ê°€ ì •í™•í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ë„ ìˆë‹¤. 

3. ì‹œê°í™”í•˜ê¸°
```
pyLDAvis.enable_notebook()
data = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)
data
```
![LDA_2](/practiceNLP/LDA_2.png)
í† í”½ë³„ë¡œ í•´ë‹¹ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ ê´€ë ¨ì„± ë“±ì„ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•´ì¤€ë‹¤. 

### 2. ELMo
ELMo(Embeddings from Language Model)ëŠ” 2018ë…„ì— ë¯¸êµ­ì˜ Allen AI ì—°êµ¬ê¸°ê´€ê³¼ ì›Œì‹±í„´ëŒ€í•™êµ ê³µë™ ì—°êµ¬íŒ€ì´ ì œì•ˆí•œ ì›Œë“œ ì„ë² ë”© ë°©ë²•ìœ¼ë¡œ, ì˜ ì•Œê³  ìˆë“¯ ë§Œí™” ì„¸ì„œë¯¸ìŠ¤íŠ¸ë¦¬íŠ¸ì˜ ìºë¦­í„° ì´ë¦„ì„ ë”°ì„œ ë§Œë“¤ì—ˆë‹¤. ELMo ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸(Pre-trained language model)ì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ë•Œ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì´ë€ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ê°’ ë˜ëŠ” ë¶€ë¶„ìœ¼ë¡œ ì¬ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì„ ë§í•˜ë©° 'ì „ì´ í•™ìŠµ'ì´ë¼ê³ ë„ í•œë‹¤. ì „ì´í•™ìŠµ(pretrain)ì´í›„ 

ELMoëŠ” ì„¸ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤.
* Convilutional Neural Network (ê° ë‹¨ì–´ ë‚´ ë¬¸ìë“¤ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ ë„ì¶œ. pretrainì—ì„œ í•™ìŠµ)
* ì–‘ë°©í–¥ LSTM ë ˆì´ì–´ (ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ ì¶”ì¶œ. pretrainì—ì„œ í•™ìŠµ)
* ELMo ë ˆì´ì–´(pretrainì´í›„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ë•Œ, ì¦‰ fine tuning í•  ë•Œ í•™ìŠµ)   

ì´ ê³¼ì •ì€ ì‚¬ì‹¤ êµ‰ì¥íˆ ì–‘ì´ ë°©ëŒ€í•˜ê³  ì „ì²´ë¥¼ ë‹¤ ì´í•´í•˜ê¸°ì—ëŠ” ìƒë‹¹í•œ ê¸°ë³¸ ì§€ì‹ê³¼ ì‹œê°„ì´ ê±¸ë¦´ ê²ƒ ê°™ë‹¤. ì „ì²´ì ì¸ íë¦„ì„ ì´í•´í•˜ê³  ë„˜ì–´ê°€ë„ë¡ í•œë‹¤. ì‹¤ìŠµí•œ ì½”ë“œëŠ”[ìœ„í‚¤ë…ìŠ¤](https://wikidocs.net/33930)ì—ì„œ ì§„í–‰í•œ ìŠ¤íŒ¸ë©”ì¼ ë¶„ë¥˜ê¸° ì½”ë“œë¥¼ í™œìš©í•˜ì˜€ìœ¼ë¯€ë¡œ ì´ ê¸€ì— ì½”ë“œë¥¼ ì¶”ê°€í•˜ì§„ ì•Šë„ë¡ í•˜ê² ë‹¤.  

ì°¸ê³ ë„ì„œ: í•œêµ­ì–´ì„ë² ë”©(ì´ê¸°ì°½, 2019) p203~218   
ì°¸ê³ ì‚¬ì´íŠ¸: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://nlpinkorean.github.io/illustrated-bert/) | [ìœ„í‚¤ë…ìŠ¤](https://wikidocs.net/33930) 



### 3. BERT
ì•ì„œ ELMoì™€ ê°™ì´ ì• ë‹ˆë§¤ì´ì…˜ ì„¸ì„œë¯¸ìŠ¤íŠ¸ë¦¬íŠ¸ì˜ ìºë¦­í„° ì´ë¦„ì„ ë¶ˆì—¬ ê·€ì—¬ìš´ ëŠë‚Œì„ ì£¼ëŠ” BERTëŠ” êµ¬ê¸€ì´ 2018ë…„ì— ê³µê°œí•˜ì˜€ê³  ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. BERTì˜ ê¸°ë²•ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ['íŠ¸ë ŒìŠ¤í¬ë¨¸'](https://wikidocs.net/31379) ë©”ì»¤ë‹ˆì¦˜ì„ ìš°ì„ ì ìœ¼ë¡œ ì´í•´í•´ì•¼ í•˜ëŠ”ë°, ë‚´ìš©ì´ ë°©ëŒ€í•˜ì—¬ ê°œë…ì— ê¹Šì— ë“¤ì–´ê°€ì§€ ì•Šê³  ê°„ë‹¨í•˜ê²Œë§Œ ì‚´í´ë³¸ë‹¤. 

BERTëŠ”ì•ì„œ ë°œí‘œëœ EMLo, GPTì™€ ë‹¬ë¦¬ ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì–‘ë°©í–¥ì„ ì§€í–¥í•œë‹¤. ì‹œí€€ìŠ¤ë¥¼ ë³´ëŠ” ì–¸ì–´ëª¨ë¸ê°™ì€ ê²½ìš°ì—ëŠ” í•œ ë°©í–¥ë§Œ ë³´ëŠ” ë°˜ë©´, BERTëŠ” ë¬¸ì¥ ì „ì²´ë¥¼ ë¨¼ì € ë³´ê¸° ë•Œë¬¸ì— ì–‘ ë°©í–¥ ëª¨ë‘ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. 

BERTëŠ” ë¬´ê±°ìš´ í¸ì´ë¼ TPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  í•œë‹¤. Colabì—ì„œ ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ì—ì„œ 'TPU' ë¡œ ì„¤ì • ë³€ê²½í•˜ê³  ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œ ì¤€ë‹¤.  (ì•„ë˜ ì‹¤ìŠµê°™ì€ ê²½ìš°ì—ëŠ” ê·¸ëƒ¥ ê¸°ì¡´ì˜ CPU ì„¤ì •ìœ¼ë¡œë„ ë¬¸ì œ ì—†ì´ ì‹¤í–‰ë˜ì—ˆë‹¤.)      
```
# TPU ì´ˆê¸°í™”
import tensorflow as tf 
import os 
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR']) 
tf.config.experimental_connect_to_cluster(resolver)   
tf.tpu.experimental.initialize_tpu_system(resolver)

# TPU strategy ì„¤ì •
strategy = tf.distribute.TPUStrategy(resolver)
```
ì½”ë“œ ì¶œì²˜: [ìœ„í‚¤ë…ìŠ¤](https://wikidocs.net/119990)

BERTëŠ” ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” í•­ìƒ ë§µí•‘ ê´€ê³„ì—¬ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ì¡°ê±´ê³¼ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê¸° ìœ„í•´ì„œëŠ” í•™ìŠµ ë°ì´í„°ê°€ ë§ì•„ì•¼ í•œë‹¤ëŠ” ì  ë“±ì˜ ì´ìœ ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ pretrainì— ì ìš©ì„ ì‹œë„í•´ë³´ì§€ ëª»í–ˆë‹¤. 

**3-1) ì¤‘ê°„ ë‹¨ì–´ ì˜ˆì¸¡í•˜ê¸° (Masked Language Model)**
FillMaskPipelineì„ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•œë‹¤.   
1. ì„¤ì¹˜í•˜ê¸°
```
!pip install transformers
```
```
from transformers import TFBertForMaskedLM
from transformers import AutoTokenizer
```
2. ì˜ˆì œ ë°ì´í„° ë‹¤ìš´ë°›ê¸°
[KLUE ](https://github.com/KLUE-benchmark/KLUE)ì—ì„œ 2021ë…„ 5ì›” ì¯¤ ë°œí‘œí•œ ë…¼ë¬¸ì— ë“¤ì–´ìˆëŠ” í•œêµ­ì–´ BERT ì´ë‹¤. 
```
model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
```
3. ì˜ˆì¸¡í•˜ê¸°
```
from transformers import FillMaskPipeline
pip = FillMaskPipeline(model=model, tokenizer=tokenizer)
```
```
pip('NLPëŠ” ì •ë§ ì–´ë ¤ìš´ [MASK]ë‹¤.')
```
![BERT_masked](/practiceNLP/BERT_masked.png)
[MASK] ë¶€ë¶„ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì¤€ë‹¤. 'ìŠ¤í¬ì¸ 'ë¥¼ ì œì™¸í•˜ê³ ëŠ” ê±°ì˜  ë¬¸ë§¥ì— ì–´ìš¸ë¦¬ëŠ” ë‹¨ì–´ë¥¼ ë„ì¶œí•œë‹¤. 

ì½”ë“œ ì¶œì²˜: [ìœ„í‚¤ë…ìŠ¤](https://wikidocs.net/152922)

**3-2) ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡í•˜ê¸° (Next Sentence Prediction)**
ì•ë¬¸ì¥ê³¼ ë’· ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ” ë‚´ìš©ì¼ ë•ŒëŠ” ìµœì¢… ë ˆì´ë¸” [0]ì´ ë„ì¶œë˜ê³  ì´ì–´ì§€ì§€ ì•ŠëŠ” ë‚´ìš©ì¼ ë•ŒëŠ” [1]ì´ ë„ì¶œëœë‹¤. 

1. ì„¤ì¹˜í•˜ê¸°
```
import tensorflow as tf from transformers import TFBertForNextSentencePrediction from transformers import AutoTokenizer
```
2. ì˜ˆì œ ë°ì´í„° ë‹¤ìš´ë°›ê¸°
```
model = TFBertForNextSentencePrediction.from_pretrained('klue/bert-base', from_pt=True)
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
```
3.  ì˜ˆì¸¡í•˜ê¸°
```
# ì´ì–´ì§€ëŠ” ë‘ ê°œì˜ ë¬¸ì¥
prompt = "NLPì˜ ì„ë² ë”© ê¸°ë²•ì—ëŠ” ë¬¸ì¥ê¸°ë°˜ ì„ë² ë”© ê¸°ë²•ì´ ìˆìŠµë‹ˆë‹¤."
next_sentence = "BERTëŠ” NLPì˜ ì„ë² ë”© ê¸°ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤."
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :', tf.math.argmax(probs, axis=-1).numpy())
```
```
# ì´ì–´ì§€ì§€ ì•ŠëŠ” ë‘ ê°œì˜ ë¬¸ì¥
prompt = "NLPì˜ ì„ë² ë”© ê¸°ë²•ì—ëŠ” ë¬¸ì¥ê¸°ë°˜ ì„ë² ë”© ê¸°ë²•ì´ ìˆìŠµë‹ˆë‹¤."
next_sentence = "ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ë§¤ìš° ì¶¥ìŠµë‹ˆë‹¤."
encoding = tokenizer(prompt, next_sentence, return_tensors='tf')

logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]

softmax = tf.keras.layers.Softmax()
probs = softmax(logits)
print('ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” :', tf.math.argmax(probs, axis=-1).numpy())
```
ìœ„ ì½”ë“œì˜ ê²°ê³¼ëŠ” [0] (ìƒê´€ ìˆìŒ)ì´, ì•„ë˜ ì½”ë“œì˜ ê²°ê³¼ëŠ” [1] (ìƒê´€ ì—†ìŒ)ìœ¼ë¡œ ëª¨ë‘ ì •í™•í•˜ê²Œ ë„ì¶œí•˜ì˜€ë‹¤. 
ì°¸ê³ ë„ì„œ: í•œêµ­ì–´ì„ë² ë”© (ì´ê¸°ì°½, 2019, p227~240)
ì°¸ê³ ì‚¬ì´íŠ¸: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://nlpinkorean.github.io/illustrated-bert/)


##  ğŸŒŠ Future tasks
ì´ë²ˆ í•™ìŠµì—ì„œëŠ” ë¬¸ì¥ê¸°ë°˜ ì„ë² ë”© ê¸°ë²•ì„ ì‚¬ìš©í•œ ì‹¤ìŠµì„ ì§„í–‰í•˜ê¸´ í–ˆì§€ë§Œ, ë‚´ë¶€ì ìœ¼ë¡œ ê³µë¶€í•˜ê³  ì´í•´í•´ì•¼ í•  ê°œë…ë“¤ì´ ë§ì´ ë‚¨ì•˜ê³ , ì´ ìŠ¤í„°ë””ì˜ ê°€ì¥ í° ëª©ì ì¸ 'ë‚´ê°€ ê°€ì§„ ë°ì´í„°ë¡œ ì‹¤ìŠµ'ì„ ì§„í–‰í•˜ì§€ ëª»í–ˆë‹¤. ë˜í•œ GPTì™€ BERTì˜ ê¸°ë°˜ì¸ Transformer networkì— ëŒ€í•œ ì´í•´ë„ í•„ìš”í•œë§Œí¼, ì‹¬í™”  ê³¼ì • ìŠ¤í„°ë””ëŠ” ì•ìœ¼ë¡œ ê³„ì† ì§„í–‰í•´ë³´ê³ ì í•œë‹¤.

NLPì˜ ë„¤ ë‹¨ê³„ ì¤‘ ì„¸ ë‹¨ê³„ì— ëŒ€í•œ ì‘ì—…ì„ ì§„í–‰í•˜ê³  ìˆë‹¤. ëª¨ë‘ ë§ˆì³¤ë‹¤. ë‹¤ìŒ ì‹¤ìŠµì—ì„œëŠ” 4,5 ì£¼ì°¨ì—ì„œ ë‹¤ë£¬ ì„ë² ë”© ê¸°ë²•ë“¤ì„ íŒŒì¸ íŠœë‹í•˜ëŠ” ë‚´ìš©ì„ ë‹¤ë£¨ë ¤ê³  í•œë‹¤. 
* íŒŒì¸ íŠœë‹ (fine-tuning): pretrainí•œ ëª¨ë¸ì„ ìš°ë¦¬ê°€ í•˜ê³ ìí•˜ëŠ” íƒœìŠ¤í¬ (downstream task)ì— ë§ê²Œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì • 

ì´í›„ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ í™œìš©ì€ ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ë‹¤. 
